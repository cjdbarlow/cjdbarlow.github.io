{
  "hash": "9be20d25c347f42c5d3db371c16079e0",
  "result": {
    "markdown": "---\ntitle: \"Best Guesses and Wordle\"\ndate: 2022-01-27\ndescription: |\n  Optimising the opening gambit in Worldle, using R.\nimage: assets/wordle.png\ncategories:\n  - R\n---\n\n\n\n\nLike much of the internet, I have been fascinated with the game [Wordle](https://www.powerlanguage.co.uk/wordle/). For those unaware, the basic premise is to guess a five letter word in less than six attempts. Unlike many such games, there is no initial clue. Your first guess is made blind, and then refined in subsequent rounds based on whether the letter was:\n\n1. In the correct position,\n2. In the word, but in the incorrect position,\n3. Not in the word.\n\nIt's a simple mechanic, and quite fun. Part of the appeal comes from the fact that it's really a logic game. Since the potential solution space becomes very small very quickly; intelligent early guesses can remove vast swathes of dictionary from consideration. So the key question is; what are the best words to guess first?\n\n---\n\n# 0. Data, and Data Cleaning\n\nThe first step is getting a list of words and cutting it down to size. A cursory inspection of the wordle javascript reveals that it includes two dictionaries. Without looking too closely, the first one appears to be a list of 2,300 or so simple words which I am assuming is an answer list, and the other is a (several times larger) list of real english words that are valid guesses but (I assume) won't ever be correct answers. To keep things sporting, we'll use the second list.\n\nFirst up, load the word list. We'll also get a count of all the letters in each word, which we'll use later.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n```\nWarning in readLines(\"assets/word_list.txt\"): incomplete final line found on\n'assets/word_list.txt'\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Libraries\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n:::\n\n```{.r .cell-code}\nlibrary(stringi)\n\n# Clean\nwords = words %>% # We've loaded this from a text file off-screen\n  filter(str_length(word) == 5) %>%\n  # There's definitely a more efficient way of doing this, but this works for now.\n  mutate(a = stri_count_fixed(word, pattern = \"a\"),\n         b = stri_count_fixed(word, pattern = \"b\"),\n         c = stri_count_fixed(word, pattern = \"c\"),\n         d = stri_count_fixed(word, pattern = \"d\"),\n         e = stri_count_fixed(word, pattern = \"e\"),\n         f = stri_count_fixed(word, pattern = \"f\"),\n         g = stri_count_fixed(word, pattern = \"g\"),\n         h = stri_count_fixed(word, pattern = \"h\"),\n         i = stri_count_fixed(word, pattern = \"i\"),\n         j = stri_count_fixed(word, pattern = \"j\"),\n         k = stri_count_fixed(word, pattern = \"k\"),\n         l = stri_count_fixed(word, pattern = \"l\"),\n         m = stri_count_fixed(word, pattern = \"m\"),\n         n = stri_count_fixed(word, pattern = \"n\"),\n         o = stri_count_fixed(word, pattern = \"o\"),\n         p = stri_count_fixed(word, pattern = \"p\"),\n         q = stri_count_fixed(word, pattern = \"q\"),\n         r = stri_count_fixed(word, pattern = \"r\"),\n         s = stri_count_fixed(word, pattern = \"s\"),\n         t = stri_count_fixed(word, pattern = \"t\"),\n         u = stri_count_fixed(word, pattern = \"u\"),\n         v = stri_count_fixed(word, pattern = \"v\"),\n         w = stri_count_fixed(word, pattern = \"w\"),\n         x = stri_count_fixed(word, pattern = \"x\"),\n         y = stri_count_fixed(word, pattern = \"y\"),\n         z = stri_count_fixed(word, pattern = \"z\"))\n```\n:::\n\n\n# 1. What Letters?\n\nOur first two words allow us to rule-in or rule-out 10 letters, which should let us clear the vowels and a few key consonants. Which ones exactly can be worked out pretty easily: what are the 10 most common letters in the Wordle dictionary?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nletters = words[-1] %>%\n  colSums() %>%\n  sort(decreasing = TRUE) %>%\n  head(10) %>%\n  names()\n```\n:::\n\n\nThis bit of code takes our letter columns (we drop the singular column containing the full word for this exercise), takes the total sum of each column (how many times each letter appearing in our word list), and sorts them in descending order. The result is a little surprising, with 'u' as the lonely vowel not making the top-ten cut.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsort(letters)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"a\" \"d\" \"e\" \"i\" \"l\" \"n\" \"o\" \"r\" \"s\" \"t\"\n```\n:::\n:::\n\n\nIf we did into this a little more, we see why:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nletters = words[-1] %>%\n  colSums() %>%\n  sort(decreasing = TRUE)\n\nletters\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   s    e    a    o    r    i    l    t    n    d    u    m    p    y    c    h \n5996 5429 5011 3684 3259 3088 2652 2566 2377 2060 2044 1660 1652 1649 1551 1371 \n   b    g    k    f    w    v    z    j    x    q \n1346 1333 1295  885  844  541  394  264  251   83 \n```\n:::\n:::\n\n\n`U` is only slightly less common than `d`; and the standard wisdom of word construction would suggest that it's better to know the presence or absence of all the vowels. We'll make an executive decision here to include it, and drop the `d` to keep up at a round 10.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nletters = words[-1] %>%\n  colSums() %>%\n  sort(decreasing = TRUE) %>%\n  head(11)\n\nletters = letters[names(letters) != \"d\"] %>%\n  names()\n\nletters\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"s\" \"e\" \"a\" \"o\" \"r\" \"i\" \"l\" \"t\" \"n\" \"u\"\n```\n:::\n:::\n\n\n# 2. What Words?\n\nThe next part is also pretty simple; we want to cut our list of all words down to just those that use the letters in our top 10. We also want to avoid any duplicates so we can cover off all of the most common letters in our first two guesses.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngood_words = words %>%\n  rowwise() %>%\n  mutate(sum = sum(across(all_of(letters)))) %>% # sum of good letters\n  ungroup() %>%\n  filter(sum == 5) %>% # drop those without a 5\n  filter(across(all_of(letters), ~ .x %in% c(1, 0)))  %>% # drop those with more than one of each letter\n  select(word)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Using `across()` in `filter()` was deprecated in dplyr 1.0.8.\nℹ Please use `if_any()` or `if_all()` instead.\n```\n:::\n\n```{.r .cell-code}\ngood_words\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 407 × 1\n   word \n   <chr>\n 1 aeons\n 2 aeros\n 3 aesir\n 4 airns\n 5 airts\n 6 aitus\n 7 aline\n 8 alist\n 9 aloes\n10 aloin\n# ℹ 397 more rows\n```\n:::\n:::\n\n\nThis is a pretty reasonable result - we've cut down our list to a tenth of the size and with a little more work we could match up pairs of words that don't contain shared letters. But, there's still a little more optimisation that we can do first.\n\n# 3. ... Where Letters?\n\nThe second (and more interesting!) part of this process is where the letters should go. Letter frequency isn't uniformly distributed throughout words; lots of words tend to end with 's', for example. Our best guesses should adjust for this too.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncommon_pos = words[1] %>%\n  separate(col = word, into = c(\"1\", \"2\", \"3\", \"4\", \"5\"), sep = c(1, 2, 3, 4), remove = FALSE) %>%\n  select(-word) %>%\n  pivot_longer(everything(), names_to = \"position\", values_to = \"letter\") %>%\n  group_by(position, letter) %>%\n  filter(letter %in% letters) %>%\n  summarise(count = n())\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`summarise()` has grouped output by 'position'. You can override using the\n`.groups` argument.\n```\n:::\n:::\n\n\nThe above block splits our word list into columns of each letter, and sums the frequency of letters in each column. We've used the full word list rather than our list of ideal words because the distribution of letters are different (and the results won't always be from the ideal words), but then filter the final list to only ideal letters.\n\nWe then plot a histogram of letter frequency by position in the word; `geom_col()` is used rather than `geom_histogram()` because we've already calculated the column count and don't need `ggplot2` to do it for us.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nletter_hist = common_pos %>%\n  ungroup() %>%\n  ggplot(aes(x = position,\n             y = count)) +\n  facet_wrap(~letter, ncol = 5) +\n  geom_col() +\n  theme_minimal()\n\nletter_hist\n```\n\n::: {.cell-output-display}\n![](wordle_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nWe can now cull our list of good words further into those with the most optimal (or, at least, not sub-optimal) letter positions. I've adjusted this manually because of the importance of vowel position, though you could define some logic to do it programmatically as well; this is left as an exercise to the reader.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_words = good_words %>%\n  separate(col = word, into = c(\"1\", \"2\", \"3\", \"4\", \"5\"), sep = c(1, 2, 3, 4), remove = FALSE) %>%\n  filter(grepl(\"l|r|s|t\", `1`),\n         grepl(\"a|l|o|r|u\", `2`),\n         grepl(\"a|i|l|n|r|u\", `3`),\n         grepl(\"a|e|i|l|n|t\", `4`),\n         grepl(\"s|n\", `5`))\n\nrmarkdown::paged_table(best_words[1])\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"word\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"lanes\"},{\"1\":\"lants\"},{\"1\":\"lares\"},{\"1\":\"laris\"},{\"1\":\"larns\"},{\"1\":\"loans\"},{\"1\":\"loins\"},{\"1\":\"loran\"},{\"1\":\"lores\"},{\"1\":\"loris\"},{\"1\":\"louis\"},{\"1\":\"louns\"},{\"1\":\"louts\"},{\"1\":\"lunas\"},{\"1\":\"lunes\"},{\"1\":\"lunts\"},{\"1\":\"lures\"},{\"1\":\"rails\"},{\"1\":\"rains\"},{\"1\":\"raits\"},{\"1\":\"rales\"},{\"1\":\"ranis\"},{\"1\":\"rants\"},{\"1\":\"rauns\"},{\"1\":\"roans\"},{\"1\":\"roils\"},{\"1\":\"roins\"},{\"1\":\"roles\"},{\"1\":\"rones\"},{\"1\":\"ronts\"},{\"1\":\"rouen\"},{\"1\":\"roues\"},{\"1\":\"rouls\"},{\"1\":\"routs\"},{\"1\":\"ruins\"},{\"1\":\"rules\"},{\"1\":\"runes\"},{\"1\":\"runts\"},{\"1\":\"sarin\"},{\"1\":\"solan\"},{\"1\":\"tails\"},{\"1\":\"tains\"},{\"1\":\"tales\"},{\"1\":\"tares\"},{\"1\":\"tarns\"},{\"1\":\"toils\"},{\"1\":\"tolan\"},{\"1\":\"tolas\"},{\"1\":\"toles\"},{\"1\":\"tones\"},{\"1\":\"toran\"},{\"1\":\"toras\"},{\"1\":\"tores\"},{\"1\":\"touns\"},{\"1\":\"trans\"},{\"1\":\"tries\"},{\"1\":\"trins\"},{\"1\":\"trues\"},{\"1\":\"tuans\"},{\"1\":\"tules\"},{\"1\":\"tunas\"},{\"1\":\"tunes\"},{\"1\":\"turns\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n## 4. Combinations\n\nLastly, we take our list of best words, cross them to make a list of all possible combinations, and then cut that list down to a combination that contains only one of each of the high-yield letters, in a high-yield position.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_combinations = list(first = best_words$word,\n               second = best_words$word) %>%\n  cross() %>%\n  map_df(as_tibble) %>%\n  mutate(word = paste(first, second, sep = \"\"),\n         a = stri_count_fixed(word, pattern = \"a\"),\n         e = stri_count_fixed(word, pattern = \"e\"),\n         i = stri_count_fixed(word, pattern = \"i\"),\n         l = stri_count_fixed(word, pattern = \"l\"),\n         n = stri_count_fixed(word, pattern = \"n\"),\n         o = stri_count_fixed(word, pattern = \"o\"),\n         r = stri_count_fixed(word, pattern = \"r\"),\n         s = stri_count_fixed(word, pattern = \"s\"),\n         t = stri_count_fixed(word, pattern = \"t\"),\n         u = stri_count_fixed(word, pattern = \"u\")) %>%\n  filter(across(-c(first, second, word), ~ . == 1)) %>%\n  select(first, second)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: `cross()` was deprecated in purrr 1.0.0.\nℹ Please use `tidyr::expand_grid()` instead.\nℹ See <https://github.com/tidyverse/purrr/issues/768>.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Using `across()` in `filter()` was deprecated in dplyr 1.0.8.\nℹ Please use `if_any()` or `if_all()` instead.\n```\n:::\n\n```{.r .cell-code}\nknitr::kable(best_combinations,\n             col.names = c(\"First Word\", \"Second Word\"))\n```\n\n::: {.cell-output-display}\n|First Word |Second Word |\n|:----------|:-----------|\n|tails      |rouen       |\n|rouen      |tails       |\n:::\n:::\n\n\nThere we have it! An unlikely combination.\n",
    "supporting": [
      "wordle_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}