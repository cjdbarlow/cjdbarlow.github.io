[
  {
    "objectID": "pages/interval.html",
    "href": "pages/interval.html",
    "title": "Bayesian Intervals",
    "section": "",
    "text": "Input the trial data into the contingency table.\nNext, set a prior probability for the control and intervention groups.\nThe sliders will set 95% of the probability mass between each extreme, with the maximum value approximately between them. Identical rates in both groups indicate clinical equipoise.\nIf required, adjust the Highest Density Interval (HDI) and Region of Practical Equivalence (ROPE) to the desired values.\n\n\nThe 95% threshold for the HDI is fundamentally arbitrary, but is used by convention.\nThe Region of Practical Equivalence is the region where the effect size is weak enough to be considered clinically negligible. This is intervention-dependent: complex or expensive treatments will generally require a wider ROPE."
  },
  {
    "objectID": "pages/interval.html#how-to-use",
    "href": "pages/interval.html#how-to-use",
    "title": "Bayesian Intervals",
    "section": "",
    "text": "Input the trial data into the contingency table.\nNext, set a prior probability for the control and intervention groups.\nThe sliders will set 95% of the probability mass between each extreme, with the maximum value approximately between them. Identical rates in both groups indicate clinical equipoise.\nIf required, adjust the Highest Density Interval (HDI) and Region of Practical Equivalence (ROPE) to the desired values.\n\n\nThe 95% threshold for the HDI is fundamentally arbitrary, but is used by convention.\nThe Region of Practical Equivalence is the region where the effect size is weak enough to be considered clinically negligible. This is intervention-dependent: complex or expensive treatments will generally require a wider ROPE."
  },
  {
    "objectID": "pages/interval.html#risk-calculator",
    "href": "pages/interval.html#risk-calculator",
    "title": "Bayesian Intervals",
    "section": "Risk Calculator",
    "text": "Risk Calculator"
  },
  {
    "objectID": "pages/bs.html",
    "href": "pages/bs.html",
    "title": "Bedside Notebook",
    "section": "",
    "text": "These are my notes built up over my anaesthetic and intensive care training, and serves as an easily-searchable personal reference when I am on the floor. As the goal is to provide guidance in uncertainty, it tends to be highly opinionated - irrespective of the level of evidence supporting those opinions. Reasonable people may disagree.\nUnlike Part One, which is (or at least, was) “complete”, this is still very much under construction. Many core topics are absent, others will be covered only partially and sometimes incorrectly, particularly if it is a topic I stumbled upon years ago and have not dusted off since.\nIn particular (and somewhat counter-intuitively), things I know quite well are often covered poorly (if at all), and may reflect the incomplete or incorrect understanding I held when I first came across them. Conversely, topics I can claim only a passing association or interest with may be covered in unrestrained detail as I need to refresh heavily each time they arise.\nIt can be viewed here."
  },
  {
    "objectID": "posts/hdi.html",
    "href": "posts/hdi.html",
    "title": "Plotting Bayesian Intervals using ggPlot",
    "section": "",
    "text": "One of the many reasons I like Bayesian inference is that key Bayesian summary statistics really lend themselves to graphical display. Well-done visualisations can demonstrate a huge amount of information in a very short amount of time, demonstrate complex relationships between different elements, and do so for audiences that don’t have a grounding in statistics. Two of these are intervals: the Highest Density Interval (HDI), and the Region of Practical Equivalence (ROPE). The HDI is a type of credible interval1, and is the narrowest range of a probability distribution containing some probability density. For example, the 95% HDI contains 95% of the area under the curve (and therefore 95% of all possible outcomes), over the narrowest range of probabilities. The ROPE is a range of effect sizes that are clinically negligible. How much the HDI and the ROPE overlap give an indication of where our belief about a given intervention should fall.\nI’ve adopted R as my statistical language of choice, and use ggPlot for all but the most simple figures (and certainly for anything is that is going in a publication). One of my disappointments with ggPlot is that although it is easy to plot a probability density using the geom_density() function, the area under some part of the curve can’t be shaded explicitly - it’s an all or nothing thing. Although a few packages (e.g. bayestestR can plot credible intervals, they don’t offer the flexibility of ggPlot in styling these figures."
  },
  {
    "objectID": "posts/hdi.html#overview",
    "href": "posts/hdi.html#overview",
    "title": "Plotting Bayesian Intervals using ggPlot",
    "section": "Overview",
    "text": "Overview\n\nUse simulation to generate a posterior distribution\nWe will use calculate the posterior distribution for the difference between groups, i.e. the posterior for the effect size of an intervention.\nCalculate the HDI on the simulated data\nDerive the function that describes the posterior distribution\nPlot the area under part of the curve using stat_function()"
  },
  {
    "objectID": "posts/hdi.html#approach-in-detail",
    "href": "posts/hdi.html#approach-in-detail",
    "title": "Plotting Bayesian Intervals using ggPlot",
    "section": "Approach in Detail",
    "text": "Approach in Detail\nFirstly, we’ll generate some data. We’ll conduct a very simple simulation - we’ll use a Beta(10, 10) distribution for both intervention and control groups, a reasonably narrow prior and an assumption of no difference between groups. Since this example is only going to look at the posterior distribution and it has a conjugate prior, we can avoid simulating the prior and just do the posterior. Before that though, we need to conduct a trial. The results are impressive: only half of the intervention group had the outcome, compared to three-quarters of the control group:\n\n\n\n\n\n\n\nBeta(10,10) Prior Distribution\n\n\n\n\n\n# Libraries\nlibrary(tidyverse)\n\n\n# Form some trial data\ntrial = tribble(\n  ~grp , ~event, ~n,\n  \"int\", 50   , 100,\n  \"con\", 75   , 100\n) |&gt;\n  column_to_rownames(\"grp\")\n\nThe beta posterior distribution incorporates the same shape values as the prior distribution, alongside the results of our trial2. We’ll merge this dataframe with the priors, and then calculate a prior for the difference of priors and posterior.\n2 This approach is overkill for this example, but I’m reusing code I’ve written for another paper.\n# Set conditions\nnSim = 10^5\n\n# Function to calculate the posterior distribution\nbeta_posterior = function(shape1, shape2, events, n.group, n.sim){\n  # Update the prior with events to get shape values for the posterior\n  post.alpha = shape1 + events\n  post.beta = shape2 + n.group - events\n  \n  # Calculate and return beta posterior\n  post = rbeta(n.sim, post.alpha, post.beta)\n  post\n}\n\n\n# Simulate posterior\nposterior = data.frame(\n  con = beta_posterior(shape1 = 10,\n                       shape2 = 10,\n                       events = trial[\"con\",]$event,\n                       n.group = trial[\"con\",]$n,\n                       n.sim = nSim),\n  int = beta_posterior(shape1 = 10,\n                       shape2 = 10,\n                       events = trial[\"int\",]$event,\n                       n.group = trial[\"int\",]$n,\n                       n.sim = nSim)\n  ) |&gt;\n      mutate(diff = int - con)\n\nNow we have a posterior distribution for the control and intervention groups, as well as the difference between them3. The next bit is where the magic happens - we’ll calculate the function that approximates the distribution of effect size results, as well as the 95% HDI.\n3 Differences should be calculated by randomly selecting one value from each distribution and taking the difference - but since each column was generated randomly we can simply subtract one from the other which makes this much easier.\nlibrary(HDInterval)\n\n# Calculate function of the difference in the posterior distribution using approxfun()\npostDiffFun = posterior |&gt;\n  pull(diff) |&gt;\n  density() |&gt;\n  approxfun()\n\n\n# Calculate the highest density interval from the raw data\nhdi = posterior |&gt;\n  select(diff) |&gt;\n  HDInterval::hdi(credMass = 0.95) |&gt;\n  unname() |&gt;\n  round(digits = 2)\n\nThe last stage is simple - plot the results. I’ve assumed a ROPE of 5% around a null effect.\n\nggplot(data = data.frame(x = c(-1, 1)),\n                      aes(x)) +\n  # Vertical line at 0\n  geom_vline(data = NULL,\n             xintercept = 0,\n             colour = \"black\",\n             alpha = 0.3) +\n  # Curve\n  stat_function(fun = postDiffFun,\n                geom = \"line\",\n                colour = \"#F8766D\") +\n  # HDI\n  stat_function(fun = postDiffFun,\n                aes(fill = \"95% HDI\",\n                    colour = \"95% HDI\"),\n                geom = \"area\",\n                xlim = c(hdi[1], hdi[2]),\n                alpha = 0.5) +\n  # ROPE\n  stat_function(fun = postDiffFun,\n                aes(fill = \"ROPE\",\n                    colour = \"ROPE\"),\n                geom = \"area\",\n                xlim = c(-0.05, 0.05),\n                alpha = 0.5) +\n  # Cosmetics\n  scale_x_continuous(expand = c(0, 0),\n                     limits = c(-0.5, 0.1),\n                     breaks = c(seq(-0.45, 0.05, 0.2), hdi, -0.05, 0, 0.05)) +\n  scale_y_continuous(limits=c(0, 8),\n                     expand=expansion(mult=c(0, 0.05))) +\n  theme_light() +\n  theme(axis.text.y = element_blank()) +\n  labs(x = expression(theta[1] - theta[2]),\n       fill = NULL,\n       colour = NULL,\n       linetype = NULL,\n       y = NULL)\n\n\n\n\n\n\n\n\nAlthough more wordy, this produces a much nicer graph than the Bayesian packages I have used and allows further manipulation of the graph using the extensive list of options provided by ggPlot."
  },
  {
    "objectID": "posts/electioneering-part-one.html",
    "href": "posts/electioneering-part-one.html",
    "title": "Electioneering: Part I",
    "section": "",
    "text": "As an Australian who has been resident in New Zealand for a couple of years I was entitled to vote in the recent general1 election, and as a big fan of liberal democracy I was keen take part and see how the whole thing worked across the ditch. Although most democracies follow a “one man, one vote” system2, the actual make-up of parliament can vary depending on the system used to translate votes into representation3. Unlike Australia which uses a Single Transferable Vote (STV) to assign seats in both the upper and lower houses, New Zealand uses Mixed Member Proportional (MMP) voting to allocate all seats in a unicameral parliament.\nI thought this was interesting, and so decided to run a few simulations on what the makeup of the Australian parliament, in particular the House of Representatives, would be under different electoral systems. The series is divided into three parts:"
  },
  {
    "objectID": "posts/electioneering-part-one.html#the-contenders",
    "href": "posts/electioneering-part-one.html#the-contenders",
    "title": "Electioneering: Part I",
    "section": "The Contenders",
    "text": "The Contenders\nWe’ll review three different voting systems:\n\nFirst Past the Post\nFPTP is a simple and widely used system. Each voter casts a single vote for their preferred candidate, and the candidate with the most votes wins, regardless of whether they have an absolute majority. This is a winner-takes-all system, and so tends to favour larger, entrenched parties. It is still widely used in many countries, such as the US, the UK, and Papua New Guinea.\n\n\nSingle Transferable Vote\nSTV is the system used in Australian elections. Candidates have to reach the quota (&gt;50% of the vote) to be elected, whilst voters preference candidates in order from most-preferred to least-preferred. The counting process occurs in a defined sequence4:\n\nAll first-preference votes are tallied\nIf a candidate reaches the quota, they win\nIf no candidate reaches the quota, then the candidate with the lowest number of votes has their votes redistributed to the remaining candidates, based on the preferences of the voter\nEach candidates votes are then tallied again, and if the quota is reached then that candidate wins\nThe redistribution-and-counting cycle continues until someone reaches quota\n\nMixed Member Proportional\nMMP is the system used in New Zealand. It is a hybrid voting method that combines elements of both FPTP and proportional representation. Two votes are cast:\n\nElectorate vote\nVote for your local electorate - this uses FPTP. New Zealand has 70 electorates, and so 70 electorate MPs.\nParty vote\nThis is a national vote which allocates a number of further seats based on the proportion of votes that each party received. The NZ parliament contains 1205 total seats, so there are an additional 50 MPs who are placed from the party vote and don’t have an electorate - these are known as ‘list’ MPs. To be eligible to receive seats from the list, a party must win an electorate seat or receive &gt;5% of the party vote.\n\n\n4 This only applies to the lower house. The senate uses a slightly different system, which isn’t relevant to us here as we’re only looking at the House of Representatives.5 -ish, see the discussion in part 2 about overhang."
  },
  {
    "objectID": "posts/electioneering-part-one.html#the-plan",
    "href": "posts/electioneering-part-one.html#the-plan",
    "title": "Electioneering: Part I",
    "section": "The Plan",
    "text": "The Plan\nThe goal is to see how the makeup of the Australian parliament differs under each of these systems. This will require:\n\n\nI’ve made an editorial decision here that code used to conduct analysis will be included in the documents but most of the code used to present analysis will be hidden. This allows the logic of the analysis to be followed along with, without drowning the reader in cosmetics.\nIf you want to see (for instance) how the tables are put together, the full source code for this pages are available on the Github repo.\n\nVoting data\nWe’ll use the 2022 federal election data, courtesy of the Australian Electoral Commission.\nA map of the electorates\nWe’ll use the 2021 Commonwealth Electoral Divisions shapefile, courtesy of the Australian Bureau of Statistics6.\nA blueprint for a MMP parliament\nWe’ll make this up.\n\n6 The ABS is definitely my favourite government department (a tough decision, obviously) - there’s a huge amount of freely available data on a dizzying number of topics that are usually amenable to merging into medical datasets - if you’re doing any multi-site or multi-state analysis it is definitely worth poking around here."
  },
  {
    "objectID": "posts/electioneering-part-one.html#data-wrangling",
    "href": "posts/electioneering-part-one.html#data-wrangling",
    "title": "Electioneering: Part I",
    "section": "Data Wrangling",
    "text": "Data Wrangling\nFirst, we’ll do the usual environment set up:\n\n# Libraries\nrequire(tidyverse)    # For data wrangling\nrequire(magrittr)     # For piping arithmetic (actually)\nrequire(sf)           # For producing maps\nrequire(ggiraph)      # For producing interactive maps without D3\nrequire(patchwork)    # For combining maps\n\n\nVoting Data\nWe’ve loaded up the voting data behind the scenes, and are going to tidy up the data types. I’m not sure if we’ll need all of these columns yet, but since there aren’t that many of them it’s convenient to just transform them all now.\nWe make the binary Elected variables into logical, and identify the key factor variables. We of course don’t need to touch the character or numeric variables as R has classified these already7.\n7 We also do a cheeky find-and-replace on the DivisionNm variable (which identifies the electorates), as special characters can play havoc when it comes time to plot maps. Replacing hyphens with an en dash insures us against typographic shenanigans. I’ve got my eye on you, Eden-Monaro.\n# Data cleaning\nvotes = votes %&gt;%\n  mutate(DivisionNm = gsub(\"-\", \"–\", DivisionNm),\n         across(ends_with(\"Elected\"), ~ ifelse(. == \"N\", FALSE, TRUE)),\n         across(c(StateAb, DivisionNm, PartyAb, CalculationType), as.factor)) %&gt;%\n  # Pivot out the vote counts and percentages\n  pivot_wider(names_from = CalculationType,\n              values_from = CalculationValue) %&gt;%\n  rename(prefCount = \"Preference Count\",\n         prefPC = \"Preference Percent\",\n         tranCount = \"Transfer Count\",\n         tranPC = \"Transfer Percent\")\n\n\n# Inspect the parties in more detail\nparties = votes %&gt;%\n  group_by(PartyAb, PartyNm) %&gt;%\n  summarise()\n\nhead(parties)\n\n\n\n\n\nPartyAb\nPartyNm\n\n\n\n\n\n\n\n\nAJP\nAnimal Justice Party\n\n\nALP\nA.L.P.\n\n\nALP\nAustralian Labor Party\n\n\nALP\nLabor\n\n\nASP\nShooters, Fishers and Farmers Party\n\n\n\n\n\n\nThe other thing that needs correcting are the party names and abbreviations as there are some clear duplicates. There’s a couple of judgement calls here, but we follow established conventions and I think the ambiguity reduction is beneficial.\n\nvotes = votes %&gt;%\n  mutate(PartyAb = PartyAb %&gt;%\n           forcats::fct_collapse(\"GRN\" = c(\"GVIC\"),\n                                 \"IND\" = c(\"\")),\n         PartyNm = PartyNm %&gt;%\n           forcats::fct_collapse(\"Independent\" = c(\"\"),\n                                 \"Labor\" = c(\"A.L.P.\", \"Australian Labor Party\"),\n                                 \"Democratic Alliance\" = c(\"Drew Pavlou Democratic Alliance\"),\n                                 \"The Greens\" = c(\"The Greens (WA)\", \"Queensland Greens\"),\n                                 \"Katter's Australian Party\" = c(\"Katter's Australian Party (KAP)\"),\n                                 \"Liberal Party\" = c(\"Liberal National Party of Queensland\"),\n                                 \"The Nationals\" = c(\"National Party\"),\n                                 \"The New Liberals\" = c(\"TNL\"),\n                                 \"Western Australia Party\" = c(\"WESTERN AUSTRALIA PARTY\")))\n\n\n\nGeometric Data\nWith the voting data cleaned, we can turn our attention to the geometric data. The first order of business is to check that the names of the electorates in our shapefile (ced) line up with the names in the voting data.\n\nced = ced %&gt;%\n  mutate(CED_NAME21 = gsub(\"-\", \"–\", CED_NAME21))\n\nx = ced$CED_NAME2\ny = unique(votes$DivisionNm)\n\nsetdiff(x, y)\nsetdiff(y, x)\nrm(x, y)\n\nAll names align except the “no usual address” geoms, which we will strip out. We’ll also transform our geographic data to use the EPSG:4326/WGS84 coordinate system. Coordinate systems are complex and interesting, it is enough for our purposes to say that this datum lets us plot latitudes and longitudes from a map without having to think too hard about it.\n\nced = ced %&gt;%\n  ### Drop the unnecessary data and restrict our map geoms to the electorates\n  select(-c(CED_CODE21, STE_CODE21, AUS_CODE21, AUS_NAME21, LOCI_URI21)) %&gt;%\n  filter(CED_NAME21 %in% votes$DivisionNm) %&gt;%\n  mutate(DivisionNm = CED_NAME21) %&gt;%\n  # Transform the CRS\n  st_transform(crs = 4326) %&gt;%\n  # Reduce the window to cover continental Australia\n  st_crop(xmin = 110, xmax = 160,\n          ymin = -45, ymax = -10)\n\nWe should also put together a list of the state capitals. Electorates are drawn with the aim of having a similar number of enrolled voters in each8, and so tend to cluster around major cities. We’ll put some insets into our graphs so changes won’t be overshadowed by the large electorates that dominate much of the landmass.\n8 Although the actual number can vary quite a bit.\n# Set geography for insets\n# A list of cities, their latitude and longitude, and the radius around them that we'll capture the electorates from\nloc = tribble(~ city, ~ lat      , ~ long     , ~ rad,\n              \"mel\" , -37.8142354, 144.9668884, 45000,\n              \"syd\" , -33.8386302, 151.0310312, 40000,\n              \"bri\" , -27.4671551, 153.0169995, 15000,\n              \"ade\" , -34.7894706, 138.5687859, 15000,\n              \"per\" , -32.0212408, 115.8735055, 15000)\n\nThe last thing we need to do for the setup is define some cosmetic options, so that each party is drawn in an appropriate colour. To spare the tedium, we’ll do this off screen. If you want to follow along, the files (and the source data) are in the Github repository.\nNext time - some actual analysis!"
  },
  {
    "objectID": "posts/shoe-rack.html",
    "href": "posts/shoe-rack.html",
    "title": "Step by Step: Custom Shoe Rack Construction",
    "section": "",
    "text": "Recently, we moved house and in the process of packing realised how much we disliked our shoe rack. Searching around for a replacement, I realised why we had it in the first place - there weren’t any other options. The ideal shoe rack should be:\nPickings from the usual suspects were less than ideal. Armed with some free time and a collection of power tools, I decided to make a better one. Searching for inspiration, I came across a build for an industrial shoe rack which I quite liked, and used as the basis for this design."
  },
  {
    "objectID": "posts/shoe-rack.html#design",
    "href": "posts/shoe-rack.html#design",
    "title": "Step by Step: Custom Shoe Rack Construction",
    "section": "Design",
    "text": "Design\nMegan’s design had several things I liked, including the sturdiness of the materials, the industrial feel, and the simplicity of construction. I didn’t like use of the strap tie (I always think these look unappealing, though I appreciate the structural necessity), the narrow width, and the blockiness of the single-plank construction. I decided to build an archway across the top of the frame, and use gas valves as a way to hold coats, keys, and bags. I also decided to replace the strap tie with a single horizontal bar at the bottom of frame, which would close the bottom of the arch whilst (ideally) remaining less obvious visually.\nThe other changes were more minor. Based on the shape of our hallway, I decided to expand the width of the shelves to ~1m. I had also planned to have three levels for storing shoes, but decided to start with two and see how it goes in practice. The design itself also needed to be flexible, as the actual size of the frame will vary slightly depending on how far the pipes are screwed into the brackets - the tighter the pipe, the shorter that section of the frame becomes.\n\n\n\n\n\n\n\nSketching out the design. The major changes in the final version were:\n\nThe width of the shelf was expanded to 270mm\nFour valves rather than three on the arch\nTwo shelves rather than three\nThe horizontal cross bar is still in situ, but one level higher.\nRemoval of the pipe nipples on the bottom of the frame\n\n\nEquipment List\n\nWorkbench\nWith vice.\nCircular saw\nOr handsaw.\nPipe wrench\nThis is essential.\nTin snips\nUseful for trimming the saddle clips, but not essential\nSpirit level\nUseful, but not essential.\nDrill and driver\nStain and varnish brush\n\n\n\nParts List\nWood:\n\n6× ~1020mm long pieces of 90x45mm framing pine\nI bought 3× 2.4m pieces, and cut them to length. The precise length will vary with the width of the frame and the size of your saddle clips.\nStain\nI used a single coat of Cabot’s Cedar Water-Based Interior Stain.\nVarnish\nI used a two coats of Cabot’s Satin Water-Based Varnish.\n\n\n\nIf you want three shelves, you’ll instead need a total of nine lengths (three per shelf) of framing pine.\n25mm Pipe:\n\n\nOne of the major advantages of using 25mm pipe for the frame is that the pieces can be used as extensions for the pipe wrench, which provides a huge increase in the amount of torque and simplifies frame assembly.\n\n1× 900mm pipe\nHorizontal support brance\n2× 600mm pipe\nForm the first part of the verticals.\n4× 300mm pipe\nWidth of each shelf.\n2× 250mm pipe\nVerticals for the front section.\n2× 150mm pipe\nVerticals for the rear section.\n8× T-piece\n2× Elbows\n6× Hex nipple\nIf you don’t want the end caps, you only need 2 nipples.\n4× End caps\nOptional. I didn’t use these in the end.\n\n\n\nIf you want three shelves, you’ll need instead need a total of:\n\n12× T-piece\n6× 300mm pipe\n6× 250mm pipe\n\n\n\n\n\nIt is important to measure the actual pipe sections before you buy them - the lengths are not always as advertised.\n\n\n\n20mm Pipe:\n\n2× 20-25mm pipe adaptor\n2× 600mm pipe\nFor the second part of the verticals.\n2× Elbow\n4× Valves\n5× 150mm pipe\nThe number and length of these sections will vary depending on the desired number and spacing of the valves.\n\nOther:\n\n12× 25mm saddle clips\n24× Sheet metal screws\nIdeal for securing the saddle clips to the pine board."
  },
  {
    "objectID": "posts/shoe-rack.html#methods",
    "href": "posts/shoe-rack.html#methods",
    "title": "Step by Step: Custom Shoe Rack Construction",
    "section": "Methods",
    "text": "Methods\nThe most difficult part of construction is frame assembly. All pipes tighten in the same direction, and pipes are not designed to create loops. This means that there is a point when completing one side of a square section of frame that tightening one joint will loosen the opposing joint. The tedium of this can be minimised1 by over-tightening one side, then engaging the opposing end and tightening that end, such that both end up moderately tightened. Maintaining a square shape whilst doing this takes some trial-and-error.\n1 But not mitigated, sadly.My second tip for assembly is to build in stages, starting with assembling the part of the frame required for the shelving. This reduces the change that you will need to go-over old ground.\n\n0. Clean the pipes\nBefore beginning, clean the grease off the pipes and remove stickers. I did this at the end, and regretted it. I found acetone necessary to dissolve the glue.\n\n\n1. Create the frame horizontals\nThe first step is to assemble the parts of the frame that the shelf rests on, as these have the least tolerance for error. Any gaps stand out, as the pipes don’t sit flush with the either end of the frame. My goal was to create 270mm of space between each T-piece (or T-piece and elbow, for the top shelf), which would snugly fit three 90mm boards.\n\n\n\n\n\nThe 20mm pipe passes neatly through the 25mm T-piece, which is a convenient way to get extra torque.\n\n\nAssembling the horizontals accurately requires a good amount of torque, generated using some combination of vice, pipe wrench, and pipe.\nFour horizontals are required:\n\nTwo with a T-piece at one end, and an elbow at the other\nFor the top shelf.\nTwo with a T-piece at both ends\nFor the bottom shelf. If you want three shelves, you’ll need four of these.\n\n\n\n\n\n\nCheck each piece for symmetry.\n\n\n\n\n2. Create the frame verticals\nThe second step is to combine the horizontal sections with vertical pipe to make two pipe-squares.\n\n\n\n\n\nDouble-check the width of the horizontals.\n\n\nTo avoid creating a pipe-trapezoid, the tightness of each vertical has to be balanced - over-tightening a join will create a shorter side. This is made more complicated by the supporting horizontal beam, as now several connections have to be adjusted to create the required length.\nBefore moving on, double-check the length of each square against the width of the boards.\nI tried a few ways of positioning the pipes to assemble a neat square, and found the system pictured below the most effective. The vice held the top (the pipe-square is upside-down in the vice) horizontal steady, and allowed the pipe-wrench to be used to adjust the tightness of all the other joins. Getting the rear vertical correctly positioned required a lot of effort - the hex nipple, T-piece, and 150mm vertical had to be maximally tightened to provide enough play to fit the 250mm vertical in place.\n\n\n\nAn almost-completed pipe-square. Note the left sided vertical is overtightened into the elbow so the T-piece can be positioned. Loosening it off will screw it into the T-piece and lock the square.\n\n\n\n\n3. Assemble the frame\nNext, connect each pipe-square (or pipe-rectangle, if you have build three shelves) with the 900mm horizontal piece. The length of this can be easily adjusted by rotating one square against the other.\n\n\n\n\n\nSizable flanges on the saddle clips.\n\n\nThis is the time to determine the final length of the boards, as it depends on both the:\n\nActual width of the frame, once assembled\nWidth of the saddle clips\nThe saddle clips can have quite large flanges, which may lead to unsightly protrusions. You can accommodate this with longer boards, or by trimming the flange with tin snips.\n\nAt this point, the assembled frame should looking something like what is below. A quick test assembly with the wood planks is useful here, particularly to determine the width of the saddle clips. I ended up molding the saddle clips in an omega shape (Ω) to reduce their width, and trimming the edges of the flange with tin snips. You can mark and drill the pilot holes for the screws at this point if you wish.\n\n\n\n4. Prepare the wood\nAt this point, we take a break from frame construction to prepare the wood. Firstly, cut the boards to the desired length.\nNext is staining. I tested a few different stains, and settled on a single coat of cedar:\n\n\n\nPine off-cuts with one to three coats of stain, either:\n\nCabot’s Cedar (top right)\nFeast & Watson Old Baltic (left)\nFeast & Watson Golden Teak (bottom right)\n\nFollowing staining comes varnishing. I was after a moderately hardy and matte-ish finish, but ended up using an indoor furniture varnish with a semi-gloss (‘Satin’) finish. I was very happy with the result.\n\n\n5. Final assembly\nLast is securing the timber to the frame, and building the arch. Securing the boards is easier upside down, and so leaving the arch until last streamlines the assembly process.\nPlace the boards for the top shelf down, and position the frame and the saddle clips. Drill the pilot holes if you haven’t done so already, and screw in the saddle clips. Note that the saddle clips don’t need to be perfectly flush with the pipe - their main job is to prevent lateral or upwards movement of the boards,\nNext, build up a foundation and screw in the second shelf.\n\n\n\nOne shelf completed, and building a foundation for the next one. Incidentally, this is the most use that my copy of Harrison’s has seen in at least a decade.\n\n\nOnce the shelves are screwed in, flip the frame to the upright position. Loosely attach the 60mm 25mm pipe, the 25-20mm adaptor, and the 60mm 20mm pipes. Assembling the valves and 125mm pipe segments takes some finesse, with the primary goal to secure each piece tightly, so that the weight of bags on the valve handle doesn’t loosen the fitting2.\n2 This may vary depending on the brand of valve, but my valves open in a direction such that the fitting pipe would be loosened if enough weight is applied."
  },
  {
    "objectID": "posts/shoe-rack.html#results",
    "href": "posts/shoe-rack.html#results",
    "title": "Step by Step: Custom Shoe Rack Construction",
    "section": "Results",
    "text": "Results\nI am quite happy with how this turned out. I elected not to put the end caps on the bottom, as I think they narrow the floor contact and will end up applying more pressure to the floorboards. The T-pieces also sit flush with the floor. It’s quite stable, and doesn’t warp when moving it around or sitting on it. At this stage I am undecided about a third shelf - the height is ideal for sitting or lacing and raising it by another ~30cm may make it uncomfortably tall.\n\n\n\nThe final product."
  },
  {
    "objectID": "posts/electioneering-part-three.html",
    "href": "posts/electioneering-part-three.html",
    "title": "Electioneering: Part III",
    "section": "",
    "text": "Last time we determined the makeup of parliament under the three different systems. This time, we’ll reframe these results so we can perform a side-to-side analysis and draw some reasonable inferences. If you’ve skipped to the end1, a reminder that the series is divided into three parts:\nDue to the presence of list seats in an MMP system, looking at electorates alone elides the comparative advantage of the MMP system. For this reason, I’ve broken this section into two subsections: the first determines the total composition of parliament under each of FPTP, MMP, and STV, whilst the second compares the winners of electorates under FPTP and STV."
  },
  {
    "objectID": "posts/electioneering-part-three.html#composition-by-party",
    "href": "posts/electioneering-part-three.html#composition-by-party",
    "title": "Electioneering: Part III",
    "section": "Composition by Party",
    "text": "Composition by Party\nFirst, we’ll pull all our results for each electorate system into a single table. We’ll also calculate the percentage difference in party vote and received seats as a marker of how well each system reflects the preferences of the electorate.\n\ncompare.parliament = votes %&gt;%\n  group_by(PartyAb) %&gt;%\n  filter(CountNumber == 0) %&gt;%\n  summarise(partyVoteN = sum(prefCount)) %&gt;%\n  mutate(partyVotePC = (partyVoteN / sum(partyVoteN)) %&gt;%\n           multiply_by(100) %&gt;%\n           round(digits = 2)) %&gt;%\n  # Join to MMP\n  full_join(mmp %&gt;%\n              select(PartyAb,\n                     mmpElecSeatsN = seatsElecN,\n                     mmpListSeatsN = seatsListN,\n                     mmpTotSeatsN = seatsTotalN,\n                     mmpTotSeatsPC = seatsTotalPC)) %&gt;%\n  # Join to STV\n  full_join(stv %&gt;%\n              group_by(PartyAb) %&gt;%\n              summarise(stvSeatsN = n())) %&gt;%\n  # Join to FPTP\n  full_join(fptp %&gt;%\n              group_by(PartyAb) %&gt;%\n              summarise(fptpSeatsN = n())) %&gt;%\n  # Tidy up the missing\n  mutate(across(c(stvSeatsN, fptpSeatsN), ~ ifelse(is.na(.), 0, .)),\n         stvSeatsPC = (stvSeatsN / sum(stvSeatsN)) %&gt;%\n                  multiply_by(100) %&gt;%\n                  round(digits = 2),\n         fptpSeatsPC = (fptpSeatsN / sum(fptpSeatsN)) %&gt;%\n           multiply_by(100) %&gt;%\n           round(digits = 2)) %&gt;%\n  # Difference from party vote\n  mutate(across(ends_with(\"SeatsPC\"), ~ . - partyVotePC, .names = \"{.col}_diff\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComparison of Parliament Under Three Voting Systems\n\n\nAustralian Federal Election 2022\n\n\nParty\nTotal Vote (%)\nMixed Member Proportional\nSingle Transferrable Vote\nFirst Past the Post\n\n\nElectorate Seats (n)\nList Seats (n)\nTotal Seats (n)\nSeats (%)\nDeviance (%)\nSeats (n)\nSeats (%)\nDeviance (%)\nSeats (n)\nSeats (%)\nDeviance (%)\n\n\n\n\nALP\n32.58\n71\n10\n81\n36.65\n4.07\n77\n50.99\n18.41\n71\n47.02\n14.44\n\n\nLP\n23.89\n40\n19\n59\n26.70\n2.81\n27\n17.88\n-6.01\n40\n26.49\n2.60\n\n\nGRN\n12.25\n2\n29\n31\n14.03\n1.78\n4\n2.65\n-9.60\n2\n1.32\n-10.93\n\n\nLNP\n8.00\n23\n0\n23\n10.41\n2.41\n21\n13.91\n5.91\n23\n15.23\n7.23\n\n\nIND\n5.30\n3\n0\n3\n1.36\n-3.94\n10\n6.62\n1.32\n3\n1.99\n-3.31\n\n\nON\n4.96\n0\n12\n12\n5.43\n0.47\n0\n0.00\n-4.96\n0\n0.00\n-4.96\n\n\nUAPP\n4.12\n0\n0\n0\n0.00\n-4.12\n0\n0.00\n-4.12\n0\n0.00\n-4.12\n\n\nNP\n3.60\n10\n0\n10\n4.52\n0.92\n10\n6.62\n3.02\n10\n6.62\n3.02\n\n\nLDP\n1.73\n0\n0\n0\n0.00\n-1.73\n0\n0.00\n-1.73\n0\n0.00\n-1.73\n\n\nAJP\n0.60\n0\n0\n0\n0.00\n-0.60\n0\n0.00\n-0.60\n0\n0.00\n-0.60\n\n\nCYA\n0.39\n0\n0\n0\n0.00\n-0.39\n0\n0.00\n-0.39\n0\n0.00\n-0.39\n\n\nKAP\n0.38\n1\n0\n1\n0.45\n0.07\n1\n0.66\n0.28\n1\n0.66\n0.28\n\n\nXEN\n0.25\n1\n0\n1\n0.45\n0.20\n1\n0.66\n0.41\n1\n0.66\n0.41\n\n\nWAP\n0.23\n0\n0\n0\n0.00\n-0.23\n0\n0.00\n-0.23\n0\n0.00\n-0.23\n\n\nGAP\n0.21\n0\n0\n0\n0.00\n-0.21\n0\n0.00\n-0.21\n0\n0.00\n-0.21\n\n\n\nNB: Parties receiving fewer than 30,000 votes are not displayed.\n\n\nDeviance is the percentage difference between the proportion of seats held compared to the 1st preference vote.\n\n\nTotal vote percentage includes independents, whilst the 5% threshold for qualification for list seats was based on party vote, which excluded independents.\n\nThis is why One Nation received list seats, despite receiving &lt;5% of the total vote.\n\n\n\n\n\n\n\n\nI was initially a bit surprised by these results, as my assumption when I started this project was that STV would be the superior system as it removes the need for tactical voting. This reflects my own biases - the MMP system is explicitly designed to produce a parliament that reflects the voting preferences of the electorate.\nWhat is impressive about the MMP system is just how well it achieves this, which suggests my subjective anxiety for tactical voting is probably overstated. With the exception of independents, who receive significantly fewer seats under the MMP system2, prominent minor parties gain a significant number of seats that better reflects voter preference. Another unexpected finding was how both FPTP and STV demonstrated similar levels of deviance.\n2 This also reflects the approach I took by explicitly disqualifying independents from competing for list seats. This compromise was made because independents don’t exist under the MMP system, but significantly the independent cohort relative to parties.One of the key limitations here is that as STV removes the benefit of tactical voting, this does not necessarily reflect what Australian voter behaviour would look like under an MMP system."
  },
  {
    "objectID": "posts/electioneering-part-three.html#composition-by-electorates",
    "href": "posts/electioneering-part-three.html#composition-by-electorates",
    "title": "Electioneering: Part III",
    "section": "Composition by Electorates",
    "text": "Composition by Electorates\nNow we will compare the outcomes of electorates under STV and FPTP, highlighting electorates where the outcome was different. As MMP uses FPTP to determine the winner of electorates, there is no additional value for including MMP in this comparison.\n\ncompare.seat = full_join(\n  stv %&gt;%\n    mutate(name = paste(GivenNm, Surname)) %&gt;%\n    select(StateAb, DivisionNm,\n           stvID = CandidateID, stvName = name, stvParty = PartyAb, stvCount = prefCount, stvMargin = margin),\n  fptp %&gt;%\n    mutate(name = paste(GivenNm, Surname)) %&gt;%\n    select(StateAb, DivisionNm,\n           fptpID = CandidateID, fptpName = name, fptpParty = PartyAb, fptpCount = prefCount, fptpMargin = margin),\n  by = c(\"StateAb\", \"DivisionNm\")) %&gt;%\n  mutate(identical = ifelse(stvID == fptpID, TRUE, FALSE)) %&gt;%\n  select(-c(stvID, fptpID))\n\n\n\n\n\n\n\n\n\nComparison of Electorates with Differing Outcomes Under STV and FPTP\n\n\nAustralian Federal Election 2022\n\n\n\nState\nSingle Transferable Vote\nFirst Past the Post\n\n\nElected Member\nParty\nVotes (n)\nMargin (n)\nElected Member\nParty\nVotes (n)\nMargin (n)\n\n\n\n\nBennelong\nNSW\nJerome LAXALE\nALP\n50,801.00\n1,954.00\nSimon KENNEDY\nLP\n41,206.00\n3,610.00\n\n\nBoothby\nSA\nLouise MILLER-FROST\nALP\n60,579.00\n7,451.00\nRachel SWIFT\nLP\n43,196.00\n6,450.00\n\n\nBrisbane\nQLD\nStephen BATES\nGRN\n58,460.00\n8,122.00\nTrevor EVANS\nLNP\n41,032.00\n11,380.00\n\n\nCurtin\nWA\nKate CHANEY\nIND\n53,847.00\n2,657.00\nCelia HAMMOND\nLP\n43,408.00\n12,466.00\n\n\nFowler\nNSW\nDai LE\nIND\n44,348.00\n2,793.00\nKristina KENEALLY\nALP\n30,973.00\n5,627.00\n\n\nGilmore\nNSW\nFiona PHILLIPS\nALP\n56,039.00\n373.00\nAndrew CONSTANCE\nLP\n46,941.00\n6,766.00\n\n\nGoldstein\nVIC\nZoe DANIEL\nIND\n51,861.00\n5,635.00\nTim WILSON\nLP\n39,607.00\n5,792.00\n\n\nHiggins\nVIC\nMichelle ANANDA-RAJAH\nALP\n49,726.00\n3,941.00\nKatie ALLEN\nLP\n38,859.00\n11,672.00\n\n\nKooyong\nVIC\nMonique RYAN\nIND\n54,276.00\n6,035.00\nJosh FRYDENBERG\nLP\n43,736.00\n2,433.00\n\n\nLyons\nTAS\nBrian MITCHELL\nALP\n37,341.00\n1,344.00\nSusie BOWER\nLP\n27,296.00\n6,001.00\n\n\nMackellar\nNSW\nSophie SCAMPS\nIND\n51,973.00\n4,955.00\nJason FALINSKI\nLP\n40,993.00\n3,269.00\n\n\nNorth Sydney\nNSW\nKylea Jane TINK\nIND\n51,392.00\n5,666.00\nTrent ZIMMERMAN\nLP\n36,956.00\n12,479.00\n\n\nRobertson\nNSW\nGordon REID\nALP\n50,277.00\n4,344.00\nLucy WICKS\nLP\n38,448.00\n2,217.00\n\n\nRyan\nQLD\nElizabeth WATSON-BROWN\nGRN\n52,286.00\n5,256.00\nJulian SIMMONDS\nLNP\n38,239.00\n8,236.00\n\n\nTangney\nWA\nSam LIM\nALP\n56,331.00\n5,114.00\nBen MORTON\nLP\n43,008.00\n2,068.00\n\n\nWentworth\nNSW\nAllegra SPENDER\nIND\n48,186.00\n7,449.00\nDave SHARMA\nLP\n35,995.00\n4,185.00\n\n\n\n\n\n\n\nThere are a couple of interesting observations here. Firstly, the teal wave would not have occurred in a FPTP system3. This may reflect the fact that centrist independents are uniquely positioned to draw preferences from both major parties and the Greens. Secondly, the L/NP benefits significantly more than Labor under FPTP. This may reflect the greater number of minor parties on the left wing, which dilutes the Labor vote.\n3 Teal candidates include Kylea Tink, Sophie Scamps, Allegra Spender, Monique Ryan and Zoe Daniel. The only teal who would have held her seat under FPTP was Zali Steggall.Again, one of the key limitations here is that as STV removes the benefit of tactical voting, first preferences may not reflect voter behaviour under FPTP.\n\nElectoral Maps\nThe below maps are very similar, reflecting that ~90% of the parliament remains unchanged in each system4. The fact that the changed electorates were in dense urban areas further adds to the unimpressiveness.\n4 This is still significant though!\nFirst Past the PostSingle Transferable Vote"
  },
  {
    "objectID": "posts/electioneering-part-three.html#conclusions",
    "href": "posts/electioneering-part-three.html#conclusions",
    "title": "Electioneering: Part III",
    "section": "Conclusions",
    "text": "Conclusions\nAfter this, I think that MMP is a better system for mapping voter preferences to parliament composition. Whilst I think STV tends to track voter preferences at an electorate level, it’s failing is that it only tips relatively unsafe seats and so only makes a difference on the margins. Conversely, MMP works at the level parliament rather than electorate level, and so better expresses views that are held by a small percentage of the population across a wide geographical area."
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "1. Sidebotham D, Barlow CJ. The central limit theorem: The remarkable theory that explains all of statistics. Anaesthesia. 2024 Sep;79(10):1117–21. Available from: https://dx.doi.org/10.1111/anae.16420\n\n\n2. Sidebotham D, Dominick F, Deng C, Barlow J, Jones PM. Statistically significant differences versus convincing evidence of real treatment effects: An analysis of the false positive risk for single-centre trials in anaesthesia. British Journal of Anaesthesia. 2024 Jan;132(1):116–23. Available from: https://dx.doi.org/10.1016/j.bja.2023.10.036\n\n\n3. Sidebotham D, Barlow CJ. The winners curse: Why large effect sizes in discovery trials always get smaller and often disappear completely. Anaesthesia. 2023 Oct; Available from: https://doi.org/10.1111%2Fanae.16161\n\n\n4. Sidebotham D, Barlow CJ, Martin J, Jones PM. Interpreting frequentist hypothesis tests: Insights from bayesian inference. Canadian Journal of Anesthesia/Journal canadien danesthésie. 2023 Oct 1;70(10):1560–75. Available from: https://doi.org/10.1007%2Fs12630-023-02557-5\n\n\n5. Seretny M, Barlow J, Sidebotham D. The credibility plot for extreme explanations and all explanations in between. Anaesthesia. 2023;78(4):535–5. Available from: https://www.doi.org/10.1111%2Fanae.15935\n\n\n6. Seretny M, Barlow J, Sidebotham D. Multicentre randomised trials in anaesthesia: An analysis using Bayesian metrics. Anaesthesia. 2023 Sep;78:73–80. Available from: https://doi.org/10.1111%2Fanae.15867\n\n\n7. Noyahr JK, Tatucu-Babet OA, Chapple LS, Barlow CJ, Chapman MJ, Deane AM, et al. Methodological rigor and transparency in clinical practice guidelines for nutrition care in critically ill adults: A systematic review using the AGREE II and AGREE-REX tools. Nutrients. 2022 Jun;14(13):2603. Available from: https://doi.org/10.3390%2Fnu14132603\n\n\n8. Sidebotham D, Barlow CJ. False-positive and false-negative risks for individual multicentre trials in critical care. BJA Open. 2022 Mar;1:100003. Available from: https://doi.org/10.1016%2Fj.bjao.2022.100003\n\n\n9. Barlow CJ, Pilcher D. Severity scoring and outcome prediction. In: Bersten AD, Handy J, editors. Oh’s intensive care manual. 8th ed. Elsevier; 2018. \n\n\n10. Barlow CJ, Morrison S, Stephens HO, Jenkins E, Bailey MJ, Pilcher D. Unprofessional behaviour on social media by medical students. Medical Journal of Australia. 2015 Dec;203(11):439–9. Available from: https://doi.org/10.5694%2Fmja15.00272\n\n\n11. Barlow CJD. Use of adrenaline in digital nerve blocks. Medical Journal of Australia. 2012 Sep;197(6):334–4. Available from: https://doi.org/10.5694%2Fmja12.10633"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "cjdbarlow.github.io",
    "section": "",
    "text": "Cardiac anaesthesia and Intensive Care registrar fellow currently living and working in Auckland, New Zealand. I like everything at the pointy end of medicine, and have eclectic interests across medical education, physical computing and biotech, and statistics and data visualisation.\nI like doing things, so please feel free to contact me about projects, ideas, or hairbrained schemes where I could have a positive impact: medical (cardiac ++, trauma +), humanitarian, challenging (but tractable), educational, stats (prediction modelling, Bayesian, and data visualisation); or some combination of the above."
  },
  {
    "objectID": "writing.html",
    "href": "writing.html",
    "title": "Miscellany",
    "section": "",
    "text": "Plotting Bayesian Intervals using ggPlot\n\n\n\n\n\n\nstatistics\n\n\nbayes\n\n\nR\n\n\nggPlot\n\n\n\nUsing ggPlot without additional packages to cleanly plot Bayesian intervals on Beta distributions, including the Highest-Density Interval and the Region of Practical Equivalence. \n\n\n\n\n\nMar 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nStep by Step: Custom Shoe Rack Construction\n\n\n\n\n\n\nDIY\n\n\n\nBetter than IKEA. \n\n\n\n\n\nFeb 25, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nReflections on the Fellowship Exam\n\n\n\n\n\n\nmedicine\n\n\n\nThoughts on how the assessment process of the CICM Fellowship Exam could be improved, developed from ideas formed during the process of exam preparation. \n\n\n\n\n\nDec 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nElectioneering: Part III\n\n\n\n\n\n\nR\n\n\nauspol\n\n\n\nFinal part a three-part series looking at how the composition of the Australian Federal parliament might vary under different electoral systems.\nIn this part, we conduct a comparative analysis of each voting system. \n\n\n\n\n\nNov 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nElectioneering: Part II\n\n\n\n\n\n\nR\n\n\nauspol\n\n\n\nPart two of a three-part series looking at how the composition of the Australian Federal parliament might vary under different electorate systems.\nIn this part, we simulate the makeup of the House of Representatives after the 2022 federal election under three different voting systems. \n\n\n\n\n\nNov 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nElectioneering: Part I\n\n\n\n\n\n\nR\n\n\nauspol\n\n\n\nFirst part of a three-part series looking at how the composition of the Australian Federal parliament might vary under different electoral systems.\nThis part provides an overview of the electoral systems under review and preparing the data for analysis. \n\n\n\n\n\nNov 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nBreaking Wordle\n\n\n\n\n\n\nR\n\n\n\nWhat are the best starting words in Wordle?\nAn approach to optimise the opening gambit, using R. \n\n\n\n\n\nJan 27, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/exam-reflections.html",
    "href": "posts/exam-reflections.html",
    "title": "Reflections on the Fellowship Exam",
    "section": "",
    "text": "I am a current CICM trainee and recently passed through the crucible of the CICM Fellowship exam. During my preparation I started to reflect on how the process might be able to be improved, and made some notes as these thoughts came to mind. What follows is the development of those ideas1.\nThese fell into two broad categories or areas of impact. The first relates to the calibration of the exam. That is, how the exam can best identify candidates who meet the relevant competency requirements. The second relate to optimising candidate performance. This involves both better preparation of trainees in general, and enabling them to perform at their best on the day. These elements are interconnected: a lack of confidence in the assessment process is a significant independent stressor that will adversely affect performance."
  },
  {
    "objectID": "posts/exam-reflections.html#exam-calibration",
    "href": "posts/exam-reflections.html#exam-calibration",
    "title": "Reflections on the Fellowship Exam",
    "section": "Exam Calibration",
    "text": "Exam Calibration\nThe aim of the CICM training program is to produce an intensivist who can perform safely and competently as a junior consultant in a general ICU. The fellowship exam is the major hurdle for program completion, and the major determinant of whether a candidate will become an ICU specialist.\nAnecdotally, many senior and very capable trainees who would (and do) make excellent intensivists repeatedly fail the exam. This strongly suggests a failure of the exam as a reliable tool for identifying candidates ready to progress in their career. It follows that candidates who fail repeatedly will have spent several years both studying and practising at a relatively senior level. It is therefore reasonable to conclude that significant number of failures are “false negatives”. A well calibrated exam will have fewer false negatives (and false positives) than one which is poorly calibrated.\nCalibration may be improved by decreasing the importance of exam technique, which has a disproportionate effect on exam success given its lack of importance in clinical practice.\n\nWritten Exam\nThe written exam tests several qualities, including:\n\nReading comprehension under intense pressure\nBreadth and depth of knowledge\nRapid recall of knowledge\nOrganisation of thought\n\nAll of these factors - except knowledge - relate to exam technique. I think the current format overvalues organisation and question comprehension at the expense of knowledge. This is not to say these other factors are unimportant - I think having an organised mind is highly desirable, but my concern with these questions is that organisation they reward structure (and exam technique) equally to knowledge, which I think should not be the intent. On one of my examiner-marked practice exams, I increased my score on a question from a four to an eight by re-organising the information. The first structure was admittedly poor, but an improvement in short-answer structure should not be worth twice as much as correct content.\nSimilarly, the length of the questions means that reading time has to be rationed as it was not possible for me to completely read either paper. Consequently, reading time is used to get a sense of where the pinch points are, rather than drafting answer structures.\nThe following suggestions aim to decrease the weight of the these other factors on the short answer questions, and therefore increase the weight given to knowledge:\n\nLimit nesting of task verbs\nQuestions that nest task verbs2 are significantly more complex than than asking either component in isolation, as they demand a more nuanced answer.\nBold the task verbs\nEmphasising the key verb reminds candidates to tailor their answer to the desired output.\nIncreasing reading time to fifteen minutes\nThe stem for many questions are much longer than the primary exam, in particular the data interpretation questions. Ten minutes is less than one minute per question - and some questions take an entire page - meaning all questions cannot be considered in detail before writing time begins.\nAllow writing on the question paper during reading time\nReading data interpretation questions would become more valuable as you can highlight key elements, such as abnormal results. This would give candidates more time to plan answers, and should result in an increase in answer quality.\nBalance depth and breadth\nMost questions should focus predominantly on a narrow range of core ICU content, and expect a high standard in these answers. Questions on rare or subspecialty areas should focus on a safe approach to an unusual scenario, with marks awarded for appropriate consultation. The paper should be balanced such that it should be possible to pass the written component with solid marks in core content questions alone. I thought the 2023.2 paper achieved this balance well.\nIncrease the number of answer booklets\nI mismanaged the numbering of booklets on multiple occasions. Several questions I wrote in their own booklet, so I was out-of-order for the requested numbering. One question I reached the end of a booklet, and wasn’t sure if I should break an answer over two booklets, so I cut my answer short and moved on. I have had this this issue in previous exams and requested extra booklets at the start but was only allocated one in the first paper and none in the second. Though I was given permission to request more as I needed them, I felt discouraged from doing so. Increasing the number of booklets to one per question would alleviate this.\n\n2 “Describe the important features of Down’s Syndrome and outline the impact they may have on his management.”\n\nHot Cases\nHot Cases are artificial scenarios, but in my view are the most valuable part of the exam. Preparation for the Hot Case has contributed the most to my improvement in bedside performance. However, I am concerned that success in this section relies too much on luck, with respect to the personal strengths of the candidate and which two cases they receive on the day. Any benefit to one candidate over another could be reduced by increasing the number of cases, say to three, and either:\n\nGrading all three cases, with failure on two of three hot cases being an overall fail, or\nDiscarding the worst score (perhaps with an exception for catastrophic errors of judgment, and grading the remaining two using the current system"
  },
  {
    "objectID": "posts/exam-reflections.html#performance-optimisation",
    "href": "posts/exam-reflections.html#performance-optimisation",
    "title": "Reflections on the Fellowship Exam",
    "section": "Performance Optimisation",
    "text": "Performance Optimisation\nCandidate performance could be improved by ensuring that trainees presenting for the exam have adequate clinical experience, understand the exam process and have confidence in the method of assessment, and have prepared appropriately for the nature of the the assessment.\n\nTrainee Readiness\nThere is an argument that trainees fail due to inadequate breadth of clinical experience. I think there may be some truth to this – the CICM exam can be sat after only ~18 months of clinical intensive care training. In fact it is optimal to do so to advance as efficiently as possible through training. But I also think this is good program design. Trainees with substantial unaccredited ICU time or clinical experience from another specialty may have sufficient knowledge to progress, whilst trainees who don’t will complete additional core time and gain knowledge there. This leads to a variable-length training program that is tailored to the learning requirements of each trainee.\nHowever, this does feel like a side effect of the design rather than a specific aim. I think trainees would benefit from better messaging and guidance on when to sit the exam from both the examiner body and supervisors of training. Both candidates and supervisors would be aided by a better understanding of which candidates succeed or fail. Evidence to support this understanding could be provided by analysis of the following data:\n\nClinical experience\n\nPost graduate experience\nYears of CICM training\nYears of ICU experience\nAnd the level of the units trained in.\nOther fellowships\n\nExam technique\n\nNumber of attempts\nNumber of questions answered/not answered\nPages per question\n\n\n\n\nPass Rate Variability\nExam pass rates vary significantly year to year, a fact that is well recognised by the College. Such variation must be attributable to either a change in the quality of the candidates, or to a change in the difficulty of the exam, or to inconsistency in the assessment process. It is unlikely that candidate variability explains a major variance in exam pass rates, and so I believe the exam is responsible. Notably, the Court appears to agree (1).\nThis change in pass rate is of major importance to trainees. Examiners prepare for the exams every year (including multiple times per year) for a 12 year stretch and may take a longer view on pass rates. Conversely, trainees prepare for a single exam with fanatical intensity over a relatively shorter time. It is devastating for them if their exam shows a &gt;30% relative reduction in pass rate compared to their peers in adjacent settings. This continually challenges the validity of the assessment process.\nWhether or not inter-exam variability in pass rates is significant, it is certainly perceived to be significant by the trainees and should be quantitatively evaluated. Approaches could include:\n\nAnalysis of exam performance data\nA published review of exam pass rates based on the characteristics of trainees, including their previous exam performance would be useful in confirming whether or not there is a significant difference in overall performance between exams, as well as possible contributing factors (e.g. pandemic effect on trainee case mix, changes in seniority of trainees, performance of different trainee demographics).\nAssessment of the effect of repeated attempts on pass rates\nDetermine whether candidates who required repeated attempts to pass the exam routinely demonstrated improvement in their scores prior to succeeding. If so, it would suggest they were sitting exams of equivalent rigour on each occasion, eventually making the threshold.\n\nIf there is significant inter-exam variation in pass rates, methods to reduce it could include:\n\nNormalisation\nAdjusting raw scores to account for variations in the difficulty of different exams, keeping a similar pass rate.\nStandardisation\nStandardisation of question difficulty by having practicing intensivists (e.g. SOTs, examiners) sitting questions under exam conditions.\n\n\n\nThe Angoff\nThe Angoff has been adopted in part as an attempt to standardise pass rates (1). As pass rates appear to have remained variable after introduction of the Angoff score, I do not think this attempt has been successful. In addition, its use generates substantial confusion on the part of trainees. “Harder” questions may also have a lower expected standard on the marking rubric, meaning that an adjustment for question difficulty is already being made by the examiners. In which case, any further adjustments will have diminishing returns.\nThe robustness of the Angoff score in this exam could be evaluated by reviewing the candidates’ scores on each question since its introduction and determining the relative pass rates (for each question and for the exam overall) for both the Angoff and the traditional standard.\n\n\nExam Resources\nThere are many resources for candidates preparing for this exam. Some of these are gated to regions, for example the Queensland trainee group, or courses (WICM, Bala’s Brisbane course) or individual units. By comparison, offerings on the College website are sparse.\nMy suggestion is to add an on-line pre-exam course to the CICM portal that should be completed within six months of applying to sit the exam. The course should cover the structure of the exam and could include much of the content that Michaela presented on Zoom over the last year.\n\n\nExam Feedback\nReturning of marked questions to unsuccessful candidates should occur. I know that this subject has been raised (repeatedly) and is said not to be feasible under the current management system. I find this explanation difficult to understand. If it is, in fact, the case, there is almost certainly a technical solution to it.\nAlong with returning of marked questions, inclusion of the median score and some measure of spread (e.g., IQR) would provide the candidate with some indication of where they sit relative to the cohort. If nothing else, this may assist them in planning of next steps."
  },
  {
    "objectID": "posts/exam-reflections.html#exam-reports",
    "href": "posts/exam-reflections.html#exam-reports",
    "title": "Reflections on the Fellowship Exam",
    "section": "Exam Reports",
    "text": "Exam Reports\nExam reports are essential to written exam preparation, and could be made better by increasing the detail of example answers. Options could include:\n\nModel answers\nI know that these have been used in the past and are now removed. But the benefits by way of insight they provided into what the examiner was looking for far outweighed any cost of candidates learning those answers by rote.\nAnonymised answers\nThe critique of anonymised answers on Zoom was very helpful to my understanding of what examiners seek in a response. I believe that providing anonymised answers corresponding to a best answer, borderline pass, and clear fail would be extremely valuable for prospective candidates. I do appreciate that a lot of work would be involved in setting this up, but it could be limited to a set of key discriminatory questions. It is likely that candidates who received the best mark for a question would not object to having their answer anonymised, which would significantly reduce the workload to the college for providing this. If available, this would be my preference over model answers."
  },
  {
    "objectID": "posts/exam-reflections.html#conclusion",
    "href": "posts/exam-reflections.html#conclusion",
    "title": "Reflections on the Fellowship Exam",
    "section": "Conclusion",
    "text": "Conclusion\nNaturally, all of the above is based on my own experiences, and is therefore biased. I also have a second fellowship, am relatively young, have good exam technique (as demonstrated by good performance on post-graduate exams), have a very supportive partner, and no children. I am not a candidate who was dogged by additional challenges or would warrant special consideration. Tailoring the exam for candidates like me may make it more poorly calibrated for the trainee body as a whole. Despite these caveats, I hope some of it is useful."
  },
  {
    "objectID": "posts/wordle.html",
    "href": "posts/wordle.html",
    "title": "Breaking Wordle",
    "section": "",
    "text": "Like much of the internet, I have been fascinated with the game Wordle. For those unaware, the basic premise is to guess a five letter word in less than six attempts. Unlike many such games, there is no initial clue. Your first guess is made blind, and then refined in subsequent rounds based on whether the letter was:\nIt’s a simple mechanic, and quite fun. Part of the appeal comes from the fact that it’s really a logic game. Since the potential solution space becomes very small very quickly; intelligent early guesses can remove vast swathes of dictionary from consideration. So the key question is; what are the best words to guess first?"
  },
  {
    "objectID": "posts/wordle.html#combinations",
    "href": "posts/wordle.html#combinations",
    "title": "Breaking Wordle",
    "section": "4. Combinations",
    "text": "4. Combinations\nLastly, we take our list of best words, cross them to make a list of all possible combinations, and then cut that list down to a combination that contains only one of each of the high-yield letters, in a high-yield position.\n\nbest_combinations = list(first = best_words$word,\n               second = best_words$word) %&gt;%\n  cross() %&gt;%\n  map_df(as_tibble) %&gt;%\n  mutate(word = paste(first, second, sep = \"\"),\n         a = stri_count_fixed(word, pattern = \"a\"),\n         e = stri_count_fixed(word, pattern = \"e\"),\n         i = stri_count_fixed(word, pattern = \"i\"),\n         l = stri_count_fixed(word, pattern = \"l\"),\n         n = stri_count_fixed(word, pattern = \"n\"),\n         o = stri_count_fixed(word, pattern = \"o\"),\n         r = stri_count_fixed(word, pattern = \"r\"),\n         s = stri_count_fixed(word, pattern = \"s\"),\n         t = stri_count_fixed(word, pattern = \"t\"),\n         u = stri_count_fixed(word, pattern = \"u\")) %&gt;%\n  filter(across(-c(first, second, word), ~ . == 1)) %&gt;%\n  select(first, second)\n\nknitr::kable(best_combinations,\n             col.names = c(\"First Word\", \"Second Word\"))\n\n\n\n\nFirst Word\nSecond Word\n\n\n\n\ntails\nrouen\n\n\nrouen\ntails\n\n\n\n\n\nThere we have it! An unlikely combination."
  },
  {
    "objectID": "posts/electioneering-part-two.html",
    "href": "posts/electioneering-part-two.html",
    "title": "Electioneering: Part II",
    "section": "",
    "text": "Previously, we conducted data cleaning and set ourselves up for a (hopefully) painless analysis. In this part, we’ll determine what the makeup of the lower house would be under each electorate system, based on the 2022 election results.\nIf you’ve arrived in medias res, a reminder that the whole series is divided into three parts:"
  },
  {
    "objectID": "posts/electioneering-part-two.html#single-transferrable-vote",
    "href": "posts/electioneering-part-two.html#single-transferrable-vote",
    "title": "Electioneering: Part II",
    "section": "Single Transferrable Vote",
    "text": "Single Transferrable Vote\nWe’ll start with Single Transferable Vote, as it is the current system in use. It’s also easy - the AEC has done all the work already.\n\nstv = votes %&gt;%\n  group_by(DivisionID) %&gt;%\n  # Take the highest count\n  slice_max(CountNumber, n = 1) %&gt;%\n  # Calculate the margin of victory\n  arrange(desc(prefCount), .by_group = TRUE) %&gt;%\n  mutate(margin = prefCount - lead(prefCount)) %&gt;%\n  # Take the winner\n  slice_max(prefCount, n = 1) %&gt;%\n  ungroup()\n\nYou can explore the outcomes for STV in the table below. Comparative analysis will have to wait for part 3."
  },
  {
    "objectID": "posts/electioneering-part-two.html#first-past-the-post",
    "href": "posts/electioneering-part-two.html#first-past-the-post",
    "title": "Electioneering: Part II",
    "section": "First Past the Post",
    "text": "First Past the Post\nFirst past the post is also straightforward - it’s whoever won the first round of preferences. We’ll calculate a couple of other metrics here as well, firstly the margin of victory to get a sense of the fragility of the system, and also put together a tooltip, which we will use when mapping this result.\n\nfptp = votes %&gt;%\n  group_by(DivisionID) %&gt;%\n  # Restrict to the first round of preferences\n  filter(CountNumber == 0) %&gt;%\n  # Calculate the margin between first and second\n  arrange(desc(prefCount), .by_group = TRUE) %&gt;%\n  mutate(margin = prefCount - lead(prefCount)) %&gt;%\n  # Identify the person with the most votes\n  slice_max(prefCount, n = 1) %&gt;%\n  mutate(tooltip = paste0(DivisionNm, \"\\n\",\n                          GivenNm, \" \", Surname, \"\\n\",\n                          PartyNm, \" by \", margin)) %&gt;%\n  ungroup()"
  },
  {
    "objectID": "posts/electioneering-part-two.html#mixed-member-proportional",
    "href": "posts/electioneering-part-two.html#mixed-member-proportional",
    "title": "Electioneering: Part II",
    "section": "Mixed Member Proportional",
    "text": "Mixed Member Proportional\nThis is the most complex of the three, as we are trying to map existing votes to a different electorate system. So, we’ll have to make some assumptions:\n\nContinue to ignore the senate, and focus solely on the lower house\nAddition of 631 list MPs, surplus2 to the current electorate MPs\n\nParty vote will be defined as the percentage of 1st preference votes that were not for an independent\nThis is probably the most objectionable - my anecdotal experience of one New Zealand election and the associated commentary is that some people may vote quite differently (including either major party) based on their relative opinion of their local candidates and the party as a whole3, as well as for strategic voting.\nIndependents are allowed to run, but can only win electorate seats, and don’t contribute to party vote4\nNew Zealand independents seem to run as a member of a single-person party, but this assumption holds in practice because they don’t receive significant proportion of the national vote and so don’t reach the 5% threshold required for list MPs.\n\n1 This makes the ratio of list:electorate MPs to be equivalent to that used by the NZ parliament, which is 50:70.2 Redrawing the current electorate boundaries to maintain 151 total seats is left as an exercise to the reader (or the AEC).3 I find this a colourable explanation - all politics is local, and popular local members seemed to hold their seats by greater margins than the national swing would assume.4 A consequence of this is all independent MPs will cause an overhang.The composition of parliament in an MMP system is done by determining:\n\nWhat the proportion of seats in parliament that each party should receive\nThe number of these seats that will be filled by electorate wins\nAdditional seats are allocated based on the ratio of electorate seats to the proportional seats.\n\nIf the number of proportionally allocated seats is greater than the number of electorate seats5, the party receives additional list seats\nIf the number of electorate seats is greater than the number of proportionally allocated seats, then this is an overhang\nIn this case, party keeps all their electorate seats and gains no list seats6.\n\n\n5 This is usual and expected.6 There are a variety of different methods to deal with an overhang, this is the method used by New Zealand, and is comparatively simple.First, a quick check to see if there are any parties that have only one member - we will count those as independents.\n\nparty.size = votes %&gt;%\n  group_by(PartyAb) %&gt;%\n  summarise(n = n()) %&gt;%\n  arrange(n)\n\nThe next step is to determine, for each party, the:\n\nNumber of electorate seats won\nProportion of the party vote received\nNumber of seats that they are entitled to\n\n\n# Calculate the number of electorate seats\n# This uses FPTP - we can reuse our above code\nmmp.elec = votes %&gt;%\n  group_by(DivisionID) %&gt;%\n  # Restrict to the first round of preferences, and then the most votes\n  filter(CountNumber == 0) %&gt;%\n  slice_max(prefCount, n = 1)\n\n\n# Calculate the total makeup of parliament\nmmp = votes %&gt;%\n  # Drop the later rounds of voting, and the independents\n  # If there were any single-member parties, we'd drop those too\n  filter(!PartyAb %in% c(\"IND\"),\n         CountNumber == 0) %&gt;%\n  droplevels() %&gt;%\n  group_by(PartyAb) %&gt;%\n  # Calculate the total party vote received (and proportion)\n  summarise(partyVote = sum(prefCount)) %&gt;%\n  mutate(partyVotePC = (partyVote / sum(partyVote)) %&gt;%\n           multiply_by(100) %&gt;%\n           round(digits = 2)) %&gt;%\n  # Add the number of electorate seats won\n  # Full join adds the independents back in, without their vote count or PC\n  full_join(mmp.elec %&gt;%\n              group_by(PartyAb) %&gt;%\n              summarise(seatsElecN = n())) %&gt;%\n  mutate(seatsElecN = replace_na(seatsElecN, 0),\n         seatsElecPC = (seatsElecN / length(levels(votes$DivisionNm))) %&gt;%\n           multiply_by(100) %&gt;%\n           round(digits = 2))\n\nNow we allocate the list MPs. Exact allocation is not possible, as MPs don’t share seats7, and so some system for distributing seats to try and minimise apportionment paradoxes8. New Zealand uses the Sainte-Laguë system, which is designed to maximise proportional representation and iteratively allocates seats based on the following rule:\n7 This could be interesting, though.8 Apportionment paradoxes occur when seat allocation is unexpected given the voting behaviour of the electorate.\\[Quotient = {Votes \\over {2 \\times Seats + 1}}\\]\nWhere:\n\nThe party with the highest \\(quotient\\) gets the next seat\n\\(Votes\\) is the number of votes that party received\nThis doesn’t change between iterations.\n\\(Seats\\) is the number of seats allocated by the system\nThis starts at 0 for all parties.\n\nWe calculate the number of MPs in our new parliament, which we could easily do by hand but instead do by piping a frankly excessive number of functions, and then calculate the quotient, which would be a pain to do by hand.\n\n# Determine number of pre-overhang seats that we will allocate to non-independents\nn.mp = votes$DivisionNm %&gt;%\n  levels() %&gt;%\n  length() %&gt;%\n  # Multiply by the ratio of NZ list:electorate MPs\n  multiply_by(1 + (50/120)) %&gt;%\n  ceiling() # 214 total seats: 63 list, 151 electorate\n\n# Initial conditions\nmmp$seatsEntitledN = 0\n\n# Determine number of entitled seats based on party vote\nfor(i in 1:n.mp){\n  mmp = mmp %&gt;%\n    # Calculate quotient\n    mutate(quot = fn.saint_lague(votes = partyVote,\n                                 seats = seatsEntitledN),\n           # Drop parties that don't receive 5% of the vote, have an electorate seat, or aren't a party\n           quot = case_when(seatsElecN == 0 & partyVotePC &lt;5 ~ NA_real_,\n                            PartyAb == \"IND\" ~ NA_real_,\n                            .default = quot))\n  \n  # Identify the party with the highest quotient, and increment their entitled seat count\n  mmp[which.max(mmp$quot),]$seatsEntitledN = mmp[which.max(mmp$quot),]$seatsEntitledN + 1\n}\n\nNow we’ll tidy this up a bit.\n\n# Calculate overhang\nmmp = mmp %&gt;%\n  mutate(overhang = ifelse((seatsElecN &gt; seatsEntitledN) & !is.na(quot), seatsElecN - seatsEntitledN, 0))\n\n\n# Determine the makeup of parliament\nmmp = mmp %&gt;%\n  mutate(seatsListN = ifelse(seatsEntitledN &gt; seatsElecN, seatsEntitledN - seatsElecN, 0),\n         seatsTotalN = seatsElecN + seatsListN,\n         # Calculate percentages\n         seatsTotalPC = (seatsTotalN / sum(seatsTotalN)) %&gt;%\n           multiply_by(100) %&gt;%\n           round(digits = 2),\n         seatsEntitledPC = (seatsEntitledN/sum(seatsEntitledN)) %&gt;%\n           multiply_by(100) %&gt;%\n           round(digits = 2)) %&gt;%\n  select(-quot) %&gt;%\n  left_join(votes %&gt;%\n              group_by(PartyAb, PartyNm) %&gt;%\n              summarise()) %&gt;%\n  relocate(seatsEntitledPC,\n           .after = \"seatsEntitledN\") %&gt;%\n  relocate(PartyNm,\n           .after = \"PartyAb\")\n\nThere’s a couple of observations that leap out to me here:\n\nWe have a moderate number of overhang seats9\n\n3 for the LNP\n1 for the nationals\n3 independents\n\nThe seat allocations nicely match the proportion of 1st preference votes\nThis really shouldn’t be that surprising (it is, after all, the system working as designed) but it is nice to see that it validates some of the other assumptions made about translating preferences to party vote.\n\n9 This ratio appears proportional to me. Given that the independents are guaranteed under the assumptions (and are therefore “free”), we end up with ~1 overhang seat per 55 total seats, which is similar to the current NZ parliament."
  },
  {
    "objectID": "posts/electioneering-part-two.html#mapping-the-results",
    "href": "posts/electioneering-part-two.html#mapping-the-results",
    "title": "Electioneering: Part II",
    "section": "Mapping the Results",
    "text": "Mapping the Results\nThe next step is to plot the result onto an interactive map of the electorates, with insets for the high density regions. Geophysical analysis is often pretty verbose, but we’ll step through it in a logical sequence. We’ll illustrate this with first past the post data.\nFirstly, we join our shapefile ced to our FPTP outcomes data. This is easy - we’ve already checked and ensured the electorate names share a common variable in both dataframes during data preparation. We then reduce the complexity of the graph10 and produce our map with standard ggplot functions.\n10 st_simplify is a handy function that combines adjacent vectors, which decreases the time it takes to produce the plot (relevant when iterating through cosmetic choices) as well as the size of the overall file. We could do this in the data cleaning stage, but I want to keep a high level of detail for the insets, so we’ll do it each time instead.\nmap.data.fptp = ced %&gt;%\n  left_join(fptp)\n\nmap.fptp = map.data.fptp %&gt;%\n  st_simplify(dTolerance = 1000) %&gt;% # We do this here because we want the insets to be high detail\n  ggplot() +\n  geom_sf_interactive(aes(fill = PartyAb,\n                          tooltip = tooltip,\n                          data_id = DivisionID),\n                      lwd = 0.05) +\n\n  # Themeing\n  scale_fill_manual(values = colourScale) +\n  labs(fill = element_blank(),\n       x = NULL,\n       y = NULL) +\n  theme_void() +\n  theme(plot.margin = unit(c(0,0,0,0), \"cm\")) # (\"left\", \"right\", \"bottom\", \"top\")\n\ngirafe(ggobj = map.fptp)\n\n\n\n\n\nNot a bad start, but as expected the districts in the urban regions are too small to be interpretable. We can improve the readability of this by creating some insets for the major cities11.\n11 We define fn.mini_map to automate this - basically it draws a circle of a given radius around a point, captures all the electorates that are fully or partially within that circle, and then draws a new map of that subset.\n# Make a mini-map for Melbourne\nmap.fptp.mel = fn.mini_map(st = map.data.fptp,\n                           lat = loc[loc$city == \"mel\",]$lat,\n                           long = loc[loc$city == \"mel\",]$long,\n                           rad = loc[loc$city == \"mel\",]$rad)\n\n# ... Repeat for the other cities\n\nWe then combine these insets with the full map to produce a hybrid map. There’s still room for improvement, but this is good enough for now I think.\n\n\n\n\n\n\nWe’ll finish up with part 3, doing some comparative analysis of each system."
  },
  {
    "objectID": "pages/bf.html",
    "href": "pages/bf.html",
    "title": "Bayesian Hypothesis Testing",
    "section": "",
    "text": "Input the trial data into the contingency table and select whether or not the results were found significant.\nNext, set the pre-test probability based on your own prior, or leave it at 0.5 if you have equipoise.\nIf necessary, adjust the Bayesian analysis strategy based on the characteristics of the trial."
  },
  {
    "objectID": "pages/bf.html#how-to-use",
    "href": "pages/bf.html#how-to-use",
    "title": "Bayesian Hypothesis Testing",
    "section": "",
    "text": "Input the trial data into the contingency table and select whether or not the results were found significant.\nNext, set the pre-test probability based on your own prior, or leave it at 0.5 if you have equipoise.\nIf necessary, adjust the Bayesian analysis strategy based on the characteristics of the trial."
  },
  {
    "objectID": "pages/bf.html#risk-calculator",
    "href": "pages/bf.html#risk-calculator",
    "title": "Bayesian Hypothesis Testing",
    "section": "Risk Calculator",
    "text": "Risk Calculator"
  },
  {
    "objectID": "pages/part_one.html",
    "href": "pages/part_one.html",
    "title": "Part One",
    "section": "",
    "text": "Part One is a reference for trainees preparing for the CICM and ANZCA Primary Exams.\n\nPart One is:\n\nDesigned to cover the assessed sections of the CICM and ANZCA curricula in enough detail to pass\nA rough guide for the expected depth of knowledge required on a topic\nA tool to correct your written answers\nA source of information you might find difficult to find elsewhere\n\nPart One is not:\n\nA textbook\n\nThe definitive guide to the primary exam\nA complete reference\nThere will be both omissions and errors. If you find any, please let me know."
  },
  {
    "objectID": "pages/part_one.html#what-it-is",
    "href": "pages/part_one.html#what-it-is",
    "title": "Part One",
    "section": "",
    "text": "Part One is a reference for trainees preparing for the CICM and ANZCA Primary Exams.\n\nPart One is:\n\nDesigned to cover the assessed sections of the CICM and ANZCA curricula in enough detail to pass\nA rough guide for the expected depth of knowledge required on a topic\nA tool to correct your written answers\nA source of information you might find difficult to find elsewhere\n\nPart One is not:\n\nA textbook\n\nThe definitive guide to the primary exam\nA complete reference\nThere will be both omissions and errors. If you find any, please let me know."
  }
]